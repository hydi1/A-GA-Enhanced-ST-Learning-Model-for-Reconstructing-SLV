import numpy as np
import pandas as pd
import torch
import random
from sklearn.model_selection import train_test_split
from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast
from torchsummary import summary

def count_param(model):
    """è®¡ç®—æ¨¡å‹çš„æ€»å¯è®­ç»ƒå‚æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def train_and_evaluate_model(seed=42):
    """
    Train and evaluate the SOFTS model using the provided training, validation, and test data.
    """
    # Configure arguments for the experiment
    args = {
        'task_name': 'uvst',
        'model_id':  f"seed{seed}",
        'model': 'GAConvGRU',
        'data': 'ssta',
        'features': 'MS',
        'learning_rate': 0.0005,
        'seq_len': 12,
        'label_len': 12,
        'pred_len': 12,
        'd_model': 64,
        'e_layers': 2,
        'd_layers': 1,
        'd_ff': 256,
        'factor': 1,
        'embed': 'timeF',
        'distil': True,
        'dropout': 0.0,
        'activation': 'gelu',
        'use_gpu': True,
        'train_epochs': 128,
        'batch_size': 16,
        'patience': 128,
        # 'loss': 'MSE',
        "use_norm": False,
        'd_core': 512,
        'freq': 'D',
        'input_size': 1056,
        'hidden_size': 64,
        'output_size': 1,  # å‡è®¾æ˜¯å›å½’ä»»åŠ¡ï¼Œè¾“å‡ºä¸€ä¸ªé¢„æµ‹å€¼
        'num_layers': 3,  # ä½¿ç”¨3å±‚GRU
        'root_path': r'D:\goole\2025115',
        "data_path": 'TSuv_data_368.npy',
        "target_path": r"D:\goole\GOPRdata\Yénan -1993_2023.xlsx",
        'target': "OT",  # OT å¯èƒ½æ˜¯ç›®æ ‡å˜é‡åç§°ï¼ˆå¦‚ Ocean Temperatureï¼‰ï¼Œéœ€ç¡®è®¤
        'seasonal_patterns': 'Monthly',
        'num_workers': 4,
        'use_amp': False,
        'output_attention': False,
        "lradj": "type1",
        'checkpoints': r'D:\project\ç»„ä»¶æ¶ˆè\convgru-ç§»é™¤Linear\SOFTS-main\checkpoints',
        "save_model": True,
        'device_ids': [0],
        'scale': True,
        'num_heads': 4,
    }

    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    # åˆå§‹åŒ–å®éªŒ
    exp = Exp_Long_Term_Forecast(args)
    # å¼€å§‹è®­ç»ƒ
    print(f"å¼€å§‹è®­ç»ƒï¼Œæ¨¡å‹ ID: {args['model_id']}, ç§å­: {seed}")
    exp.train(args)
    print("è®­ç»ƒå®Œæˆï¼")

    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    print("å¼€å§‹åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°...")
    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}'.format(
        args['task_name'],
        args['model_id'],
        args['model'],
        args['data'],
        args['features'],
        args['seq_len'],
        args['label_len'],
        args['pred_len'],
        args['d_model'],
        args['e_layers'],
        args['d_layers'],
        args['d_ff'],
        args['factor'],
        args['embed'],
        args['distil'],
        args['target']
    )
    result = exp.test(setting)

    # åˆå§‹åŒ–å®éªŒ
    exp = Exp_Long_Term_Forecast(args)
    print(f"å¼€å§‹è®­ç»ƒï¼Œæ¨¡å‹ ID: {args['model_id']}, ç§å­: {seed}")

    # æ˜¾å¼æ„å»ºæ¨¡å‹ä»¥è®¿é—®å®ƒ
    model = exp._build_model()

    # è®¡ç®—å¹¶æ‰“å°å‚æ•°é‡
    print("æ€»å¯è®­ç»ƒå‚æ•°é‡ï¼š", count_param(model))

    # è·å–ä¸€ä¸ªçœŸå®çš„è¾“å…¥æ‰¹æ¬¡
    train_data, train_loader = exp._get_data(flag='train')
    batch = next(iter(train_loader))
    batch_x, batch_y, batch_x_mark, batch_y_mark = batch

    # ç§»åŠ¨åˆ°ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡
    device = torch.device('cuda' if args['use_gpu'] and torch.cuda.is_available() else 'cpu')
    batch_x = batch_x.float().to(device)
    batch_x_mark = batch_x_mark.float().to(device) if batch_x_mark is not None else None
    batch_y = batch_y.float().to(device)
    batch_y_mark = batch_y_mark.float().to(device) if batch_y_mark is not None else None

    # æ„é€  x_decï¼ˆæ ¹æ®ä½ çš„ forward æ–¹æ³•é€»è¾‘ï¼‰
    dec_inp = batch_y

    # ä½¿ç”¨ input_data ä¼ é€’å®é™…è¾“å…¥
    input_data = (batch_x, batch_x_mark, dec_inp, batch_y_mark)
    # print(summary(model, input_data=input_data))

    # å¼€å§‹è®­ç»ƒ
    exp.train(args)
    print("è®­ç»ƒå®Œæˆï¼")

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    print("å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° (å»é‡å å…¨å±€æŒ‡æ ‡)...")
    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}'.format(
        args['task_name'],
        args['model_id'],
        args['model'],
        args['data'],
        args['features'],
        args['seq_len'],
        args['label_len'],
        args['pred_len'],
        args['d_model'],
        args['e_layers'],
        args['d_layers'],
        args['d_ff'],
        args['factor'],
        args['embed'],
        args['distil'],
        args['target']
    )

    # è·å– test è¿”å›çš„æŒ‡æ ‡
    result = exp.test(setting)
    rmse_norm_full = result.get('rmse_norm_full')
    mae_norm_full = result.get('mae_norm_full')
    r2_eff_norm_full = result.get('r2_eff_norm_full')
    rmse_denorm_full = result.get('rmse_denorm_full')
    mae_denorm_full = result.get('mae_denorm_full')

    print(f"\nç§å­ {seed} è¯„ä¼°å®Œæˆï¼")
    print(f"[Normalized]    RMSE: {rmse_norm_full:.4f}, MAE: {mae_norm_full:.4f}, R2_eff: {r2_eff_norm_full:.4f}")
    print(f"[De-normalized] RMSE: {rmse_denorm_full:.4f}, MAE: {mae_denorm_full:.4f}")

    return rmse_norm_full, mae_norm_full, r2_eff_norm_full, rmse_denorm_full, mae_denorm_full

if __name__ == "__main__":
    # æµ‹è¯•å¤šä¸ªç§å­100, 200, 300, 400, 500,
    #         123, 234, 345, 456, 567,
    #         678, 789, 888, 999, 1000,
    #         1111, 1222, 1333, 1444, 1555,
    #         1666, 1777, 1888, 1999, 2024,
    #         2025, 2048, 2077, 2099, 2121,
    #         2222, 2333, 2444, 2555, 2666,
    #         2777, 2888, 2999, 3001, 3333,
    #         3456, 3579, 3690, 4040, 5050
    seed_list = [42, 43, 44, 45, 46,
                 100, 200, 300, 400, 500,
                123, 234, 345, 456, 567,
                678, 789, 888, 999, 1000,
        ]  # æµ‹è¯•1ä¸ªç§å­ï¼Œå¯æ ¹æ®éœ€è¦å¢åŠ æ›´å¤šç§å­
    
    results = []
    for seed in seed_list:
        # æ¥æ”¶ test è¿”å›çš„5ä¸ªæŒ‡æ ‡
        rmse_norm, mae_norm, r2_eff_norm_full, rmse_denorm, mae_denorm = train_and_evaluate_model(seed=seed)
        results.append((seed, rmse_norm, mae_norm, r2_eff_norm_full, rmse_denorm, mae_denorm))

    # --- æ€§èƒ½åˆ†æä¸è¾“å‡º ---
    # 1. æŒ‰ RMSE(norm) å‡åºæ’åºï¼Œæ‰¾åˆ°æœ€å¥½çš„ç§å­ï¼ˆRMSE è¶Šå°è¶Šå¥½ï¼‰
    results.sort(key=lambda x: x[1])
    # results entries: (seed, rmse_norm, mae_norm, r2_eff_norm_full, rmse_denorm, mae_denorm)
    best_seed, b_rmse_norm, b_mae_norm, b_r2_eff, b_rmse_denorm, b_mae_denorm = results[0]

    print("\n" + "=" * 110)
    print(f"{'æ‰€æœ‰ç§å­æµ‹è¯•ç»“æœ (æŒ‰ RMSE(norm) æ’åº)':^110}")
    print("-" * 110)
    print(f"{'ç§å­':<8} | {'RMSE(norm)':<12} | {'MAE(norm)':<12} | {'R2_eff(norm)':<16} | {'RMSE(denorm)':<14} | {'MAE(denorm)':<12}")
    print("-" * 110)
    for seed, rn, mn, r2eff, rd, md in results:
        print(f"{seed:<8} | {rn:<12.4f} | {mn:<12.4f} | {r2eff:<16.4f} | {rd:<14.4f} | {md:<12.4f}")
    print("-" * 110)

    print(f"\nğŸ¥‡ æœ€ä½³ç§å­: {best_seed}")
    print(f"  > [Normalized]    RMSE: {b_rmse_norm:.4f}, MAE: {b_mae_norm:.4f}, R2_eff: {b_r2_eff:.4f}")
    print(f"  > [De-normalized] RMSE: {b_rmse_denorm:.4f}, MAE: {b_mae_denorm:.4f}")

    # 2. è®¡ç®—æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡çš„å¹³å‡æ€§èƒ½
    rmse_norms = np.array([r[1] for r in results])
    mae_norms = np.array([r[2] for r in results])
    r2_effs = np.array([r[3] for r in results])
    rmse_denorms = np.array([r[4] for r in results])
    mae_denorms = np.array([r[5] for r in results])

    print("\n" + "=" * 70)
    print(f"{'å¹³å‡æ€§èƒ½ç»Ÿè®¡ (æ‰€æœ‰ç§å­)':^70}")
    print("-" * 70)
    print(f"[Normalized+å»é‡å ] RMSE: {np.mean(rmse_norms):.4f} Â± {np.std(rmse_norms, ddof=1):.4f}")
    print(f"[Normalized+å»é‡å ] MAE:  {np.mean(mae_norms):.4f} Â± {np.std(mae_norms, ddof=1):.4f}")
    print(f"[Normalized+å»é‡å ] R2_eff: {np.mean(r2_effs):.4f} Â± {np.std(r2_effs, ddof=1):.4f}")
    print(f"\n[De-normalized] RMSE: {np.mean(rmse_denorms):.4f} Â± {np.std(rmse_denorms, ddof=1):.4f}")
    print(f"[De-normalized] MAE:  {np.mean(mae_denorms):.4f} Â± {np.std(mae_denorms, ddof=1):.4f}")
    print("=" * 70)