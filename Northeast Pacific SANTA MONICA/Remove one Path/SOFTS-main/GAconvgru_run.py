import numpy as np
import pandas as pd
import torch
import random
from sklearn.model_selection import train_test_split
from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast
from torchsummary import summary


def count_param(model):
    """è®¡ç®—æ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°æ€»æ•°"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def train_and_evaluate_model(seed=42):
    """
    Train and evaluate the SOFTS model using the provided training, validation, and test data.
    """
    # set seeds for reproducibility
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    # Configure arguments for the experiment
    args = {
        'task_name': 'vst',
        'model_id': f"seed{seed}",
        'model': 'GAconvGRU',
        'data': 'ssta',
        'features': 'MS',
        'learning_rate': 0.0005,
        'seq_len': 12,
        'label_len': 12,
        'pred_len': 12,
        'd_model': 64,
        'e_layers': 2,
        'd_layers': 1,
        'd_ff': 256,
        'factor': 1,
        'embed': 'timeF',
        'distil': True,
        'dropout': 0.0,
        'activation': 'gelu',
        'use_gpu': True,
        'train_epochs': 128,
        'batch_size': 16,
        'patience': 128,
        # 'loss': 'MSE',
        "use_norm": False,
        'd_core': 512,
        'freq': 'D',
        'input_size': 1056,
        'hidden_size': 64,
        'output_size': 1,  # å‡è®¾æ˜¯å›å½’ä»»åŠ¡ï¼Œè¾“å‡ºä¸€ä¸ªé¢„æµ‹å€¼
        'num_layers': 3,  # ä½¿ç”¨3å±‚GRU
        'root_path': r'D:\goole\2025115',
        "data_path": 'TSuv_data_368.npy',
        "target_path": r"D:\goole\GOPRdata\Yénan -1993_2023.xlsx",
        'target': "OT",  # OT å¯èƒ½æ˜¯ç›®æ ‡å˜é‡åç§°ï¼ˆå¦‚ Ocean Temperatureï¼‰ï¼Œéœ€ç¡®è®¤
        'seasonal_patterns': 'Monthly',
        'num_workers': 4,
        'use_amp': False,
        'output_attention': False,
        "lradj": "type1",
        # 'learning_rate': 0.0001,
        'checkpoints': r'D:\project\ç»„ä»¶æ¶ˆè\convgru-ç§»é™¤path1\SOFTS-main\checkpoints',
        "save_model": True,
        'device_ids': [0],
        'scale': True,
        'num_heads': 4,
    }

    # åˆå§‹åŒ–å®éªŒ
    exp = Exp_Long_Term_Forecast(args)
    print(f"å¼€å§‹è®­ç»ƒï¼Œæ¨¡å‹ ID: {args['model_id']}, ç§å­: {seed}")

    # æ˜¾å¼æ„å»ºæ¨¡å‹ä»¥è®¿é—®å®ƒ
    model = exp._build_model()

    # è®¡ç®—å¹¶æ‰“å°å‚æ•°é‡
    print("æ€»å¯è®­ç»ƒå‚æ•°é‡ï¼š", count_param(model))

    # è·å–ä¸€ä¸ªçœŸå®çš„è¾“å…¥æ‰¹æ¬¡
    train_data, train_loader = exp._get_data(flag='train')
    batch = next(iter(train_loader))
    batch_x, batch_y, batch_x_mark, batch_y_mark = batch

    # ç§»åŠ¨åˆ°ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡
    device = torch.device('cuda' if args['use_gpu'] and torch.cuda.is_available() else 'cpu')
    batch_x = batch_x.float().to(device)
    batch_x_mark = batch_x_mark.float().to(device) if batch_x_mark is not None else None
    batch_y = batch_y.float().to(device)
    batch_y_mark = batch_y_mark.float().to(device) if batch_y_mark is not None else None

    # æ„é€  x_decï¼ˆæ ¹æ®ä½ çš„ forward æ–¹æ³•é€»è¾‘ï¼‰
    dec_inp = batch_y

    # ä½¿ç”¨ input_data ä¼ é€’å®é™…è¾“å…¥
    input_data = (batch_x, batch_x_mark, dec_inp, batch_y_mark)
    # print(summary(model, input_data=input_data))

    exp.train(args)
    print("è®­ç»ƒå®Œæˆï¼")
    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    print("å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° (å»é‡å å…¨å±€æŒ‡æ ‡)...")
    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}'.format(
        args['task_name'],
        args['model_id'],
        args['model'],
        args['data'],
        args['features'],
        args['seq_len'],
        args['label_len'],
        args['pred_len'],
        args['d_model'],
        args['e_layers'],
        args['d_layers'],
        args['d_ff'],
        args['factor'],
        args['embed'],
        args['distil'],
        args['target']
    )

    # è·å– test è¿”å›çš„æŒ‡æ ‡ï¼ˆåŒ…å«å½’ä¸€åŒ–ä¸åå½’ä¸€åŒ–ï¼‰
    result = exp.test(setting)
    rmse_norm = result.get('rmse_norm', None)
    mae_norm = result.get('mae_norm', None)
    r2_eff_norm_full = result.get('r2_eff_norm_full', None)
    rmse = result.get('rmse', None)
    mae = result.get('mae', None)

    print(f"ç§å­ {seed} è¯„ä¼°å®Œæˆï¼RMSE(norm): {rmse_norm:.4f}, MAE(norm): {mae_norm:.4f}, R2_eff(norm,full): {r2_eff_norm_full:.4f} | RMSE: {rmse:.4f}, MAE: {mae:.4f}")
    return rmse_norm, mae_norm, r2_eff_norm_full, rmse, mae

if __name__ == "__main__":

#     1111, 1222, 1333, 1444, 1555,
#     1666, 1777, 1888, 1999, 2024,
#     2025, 2048, 2077, 2099, 2121,
# 2222, 2333, 2444, 2555, 2666,
# 2777, 2888, 2999, 3001, 3333,
# 3456, 3579, 3690, 4040, 5050
    seed_list = [
            42, 43, 44, 45, 46,
            100, 200, 300, 400, 500,
             123, 234, 345, 456, 567,
            678, 789, 888, 999, 1000,

    ]

    results = []
    for seed in seed_list:
        # æ¥æ”¶ test è¿”å›çš„äº”ä¸ªæŒ‡æ ‡ï¼ˆrmse_norm, mae_norm, r2_eff_norm_full, rmse, maeï¼‰
        rmse_norm, mae_norm, r2_eff_full_norm, rmse, mae = train_and_evaluate_model(seed=seed)
        results.append((seed, rmse_norm, mae_norm, r2_eff_full_norm, rmse, mae))
    # --- æ€§èƒ½åˆ†æä¸è¾“å‡º ---
    # 1. æŒ‰ RMSE(norm) å‡åºæ’åºï¼Œæ‰¾åˆ°æœ€å¥½çš„ç§å­ï¼ˆRMSE è¶Šå°è¶Šå¥½ï¼‰
    results.sort(key=lambda x: x[1])
    # results entries: (seed, rmse_norm, mae_norm, r2_eff_norm_full, rmse, mae)
    best_seed, b_rmse_norm, b_mae_norm, b_r2_eff, b_rmse, b_mae = results[0]

    print("\n" + "="*95)
    print(f"{'æ‰€æœ‰ç§å­æµ‹è¯•ç»“æœ (æŒ‰ RMSE(norm) æ’åº)':^95}")
    print("-" * 95)
    print(f"{'ç§å­':<8} | {'RMSE(norm)':<12} | {'MAE(norm)':<12} | {'R2_eff(norm,full)':<16} | {'RMSE':<12} | {'MAE':<12}")
    print("-" * 95)
    for seed, rn, mn, r2eff, r, m in results:
        print(f"{seed:<8} | {rn:<12.4f} | {mn:<12.4f} | {r2eff:<16.4f} | {r:<12.4f} | {m:<12.4f}")
    print("-" * 95)

    print(f"ğŸ¥‡ æœ€ä½³ç§å­: {best_seed}")
    print(f"  > RMSE(norm): {b_rmse_norm:.4f}, MAE(norm): {b_mae_norm:.4f}, R2_eff(norm,full): {b_r2_eff:.4f}")
    print(f"  > RMSE: {b_rmse:.4f}, MAE: {b_mae:.4f}")

    # 2. è®¡ç®—æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡çš„å¹³å‡æ€§èƒ½
    # åˆ†åˆ«è®¡ç®—å½’ä¸€åŒ–ä¸åå½’ä¸€åŒ–æŒ‡æ ‡çš„ç»Ÿè®¡
    rmse_norms = [r[1] for r in results]
    mae_norms = [r[2] for r in results]
    r2_effs = [r[3] for r in results]
    rmses = [r[4] for r in results]
    maes = [r[5] for r in results]

    print("\n" + "="*50)
    print(f"{'å¹³å‡æ€§èƒ½ç»Ÿè®¡ (æ‰€æœ‰ç§å­)':^50}")
    print("-" * 50)
    print(f"å½’ä¸€åŒ–+å»é‡å RMSE: {np.mean(rmse_norms):.4f} Â± {np.std(rmse_norms, ddof=1):.4f}")
    print(f"å½’ä¸€åŒ–+å»é‡å  MAE:  {np.mean(mae_norms):.4f} Â± {np.std(mae_norms, ddof=1):.4f}")
    print(f"R2_eff(å½’ä¸€åŒ–+å»é‡å ): {np.mean(r2_effs):.4f} Â± {np.std(r2_effs, ddof=1):.4f}")
    print(f"åå½’ä¸€åŒ– RMSE: {np.mean(rmses):.4f} Â± {np.std(rmses, ddof=1):.4f}")
    print(f"åå½’ä¸€åŒ– MAE:  {np.mean(maes):.4f} Â± {np.std(maes, ddof=1):.4f}")
    print("="*50)